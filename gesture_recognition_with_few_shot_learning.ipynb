{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**** Execute the following cell if running on Google Colab ****"
      ],
      "metadata": {
        "id": "0gxfRHUYn6FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval\n",
        "!pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CkzBLtzrQXc",
        "outputId": "e4786de2-4d96-474b-e688-2bd4c6e141ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n",
            "Installing collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n",
            "Collecting av\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-11.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rkHwAszrXwW",
        "outputId": "9a15b715-b447-4b81-d34e-76c7bcc61b13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "LbAR_yGQoQAD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9zyAsSCMq3XH"
      },
      "outputs": [],
      "source": [
        "import os, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.io.video import read_video\n",
        "from torchvision.models.video import s3d, S3D_Weights#r3d_18, R3D_18_Weights #swin3d_b, Swin3D_B_Weights\n",
        "import torch\n",
        "import torch.nn.functional as F2\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.io.video import read_video\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, utils\n",
        "from torcheval.metrics import MulticlassAccuracy, MulticlassConfusionMatrix\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ranZF-q3XK"
      },
      "source": [
        "# Creating Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uqe5fihmq3XM"
      },
      "outputs": [],
      "source": [
        "class GestureDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, device='cpu'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "        self.annotations = pd.read_csv(os.path.join(self.root_dir,'annotations.csv'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path_to_video, label = self.annotations.iloc[idx]\n",
        "        frames, _, _ = read_video(os.path.join(self.root_dir, path_to_video), output_format=\"TCHW\")\n",
        "        total_frames = len(frames)\n",
        "        desired_num_frames = 128\n",
        "        if (total_frames - desired_num_frames) == 1:\n",
        "            frames = frames[1:]\n",
        "        elif (total_frames - desired_num_frames) > 1:\n",
        "            start_offset, end_offset = self.more_frames(total_frames, desired_num_frames)\n",
        "            frames = frames[start_offset:-end_offset]\n",
        "        elif total_frames < desired_num_frames:\n",
        "            print(\"not enough frames\")\n",
        "            return\n",
        "\n",
        "        if self.transform:\n",
        "            #\n",
        "            frames = self.transform(frames)\n",
        "\n",
        "        return {'video_frames': frames, 'label': label}\n",
        "\n",
        "    def more_frames(self, total_frames, desired_num_frames):\n",
        "        start_offset = math.ceil((total_frames - desired_num_frames)/2)\n",
        "        end_offset = math.floor((total_frames - desired_num_frames)/2)\n",
        "        return start_offset, end_offset\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        videos = []\n",
        "        targets = []\n",
        "\n",
        "        for b in batch:\n",
        "            videos.append(b['video_frames'])\n",
        "            targets.append(b['label'])\n",
        "\n",
        "\n",
        "        videos = torch.stack(videos, dim=0).type(torch.float32)\n",
        "\n",
        "        return {'videos': videos, 'labels': targets}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkXphzbbq3XM"
      },
      "source": [
        "# Defining the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7Dbra1ZMq3XM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class s3d_Gestures(nn.Module):\n",
        "    def __init__(self, pretrained_model):\n",
        "        super(s3d_Gestures, self).__init__()\n",
        "        self.pretrained_model = nn.Sequential(*(list(pretrained_model.children())))\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(6000, 64),\n",
        "            nn.ReLU(),\n",
        "            # nn.Linear(64, 64),\n",
        "            # nn.ReLU(),\n",
        "        )\n",
        "        self.final_classifier = nn.Sequential(\n",
        "            nn.Linear(64, 3),\n",
        "            nn.Softmax(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pretrained_model(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.final_classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the pre-trained Separable 3D CNN model from PyTorch"
      ],
      "metadata": {
        "id": "1n3QmZAUolGw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnTvrE5eq3XN",
        "outputId": "ed57d53a-2d75-4551-a3df-5c4a564f34ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/s3d-d76dad2f.pth\" to /root/.cache/torch/hub/checkpoints/s3d-d76dad2f.pth\n",
            "100%|██████████| 32.0M/32.0M [00:00<00:00, 97.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loading done\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Initialize model with the best available weights\n",
        "weights = S3D_Weights.DEFAULT\n",
        "pretrained_model = s3d(weights=weights)\n",
        "print(\"model loading done\")\n",
        "#model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "\n",
        "*** Only run this section to train the model. Otherwise skip to next section to test the model performance on test data and get predictions from the model. ***"
      ],
      "metadata": {
        "id": "9fBhoEkqqspy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW5pr_dcq3XN"
      },
      "source": [
        "## Setting up DataLoaders and Dataset for training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *******Set path of the the root directory containing train, test and eval folders*******"
      ],
      "metadata": {
        "id": "N2Twejvao4Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/drive/MyDrive/gestures_dataset_new/\""
      ],
      "metadata": {
        "id": "LMWTm_JVo1yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing DataLoader for training the deep learning model"
      ],
      "metadata": {
        "id": "mWVX0EYXpJeZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ktiu_zYq3XN"
      },
      "outputs": [],
      "source": [
        "################# PLEASE SET THE BATCH SIZE ##################\n",
        "batch_size = 8\n",
        "##############################################################\n",
        "\n",
        "gesture_dataset_train = GestureDataset(root_dir=os.path.join(root_dir, 'train'), transform=weights.transforms(), device=device)\n",
        "train_dataloader = DataLoader(gesture_dataset_train, batch_size=batch_size, collate_fn=gesture_dataset_train.collate_fn, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kncl5BSFq3XO"
      },
      "source": [
        "## Initializing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfQevv7Hq3XP"
      },
      "outputs": [],
      "source": [
        "gesture_model = s3d_Gestures(pretrained_model).to(device)\n",
        "for param in gesture_model.pretrained_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gesture_model.train()\n",
        "for param in gesture_model.parameters():\n",
        "    print(param.requires_grad)"
      ],
      "metadata": {
        "id": "Yv_SbPT3E6qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMl8dRIsq3XP"
      },
      "source": [
        "## Traning the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "dZ0QJRdh1d01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNGy8WPPq3XP"
      },
      "outputs": [],
      "source": [
        "max_epochs = 25\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW([p for p in gesture_model.parameters() if p.requires_grad], lr=1e-3)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer,\n",
        "    T_0=max_epochs+25,\n",
        "    T_mult=1,\n",
        "    verbose=True\n",
        ")\n",
        "gesture_model.train()\n",
        "training_losses = []\n",
        "for epoch in tqdm(range(max_epochs)):\n",
        "    train_loss = 0.0\n",
        "    training_outputs = []\n",
        "    training_labels = []\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        videos, labels = data['videos'], data['labels']\n",
        "        labels = torch.tensor(F2.one_hot(torch.tensor(labels, dtype=torch.long), num_classes=3), dtype=torch.float32)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = gesture_model(videos.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(\"Loss: \", loss.item())\n",
        "        training_labels.append(labels)\n",
        "        training_outputs.append(outputs)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    training_losses.append(train_loss/len(train_dataloader))\n",
        "    print(\"Epoch: {}/{} | Training Loss: {}\".format(epoch+1,max_epochs, train_loss/len(train_dataloader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the trained model"
      ],
      "metadata": {
        "id": "ZbQTLVr-qjM6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMxCdDjeq3XP"
      },
      "outputs": [],
      "source": [
        "# you can change the name of model and save path of the model as required\n",
        "################# PLEASE SET THE PATH WHERE TO SAVE THE MODEL ##################\n",
        "torch.save(gesture_model, os.path.join(root_dir,\"gesture_model_v3.pt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDQIFmb4q3XP"
      },
      "source": [
        "# Testing the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the trained model"
      ],
      "metadata": {
        "id": "Bztp-pEPrKBl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPiTjDofq3XQ"
      },
      "outputs": [],
      "source": [
        "# Update the path of the model as required\n",
        "################# SET THE PATH TO THE SAVED MODEL ##################\n",
        "trained_model = torch.load(os.path.join(root_dir,\"gesture_model_v3.pt\"), map_location=torch.device('cpu')) # Use 'cuda' instead of 'cpu' if you saved the model on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMSXk1Vkq3XQ"
      },
      "outputs": [],
      "source": [
        "trained_model = trained_model.to(device)\n",
        "trained_model = trained_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creatingt the test dataloader"
      ],
      "metadata": {
        "id": "3auCEeATrYeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "gesture_dataset_test = GestureDataset(root_dir=os.path.join(root_dir, 'test'), transform=weights.transforms(), device=device)\n",
        "test_dataloader = DataLoader(gesture_dataset_test, batch_size=batch_size, collate_fn=gesture_dataset_train.collate_fn, shuffle=True)"
      ],
      "metadata": {
        "id": "lfYARSgOtLME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observing performance of the model on batches of test data"
      ],
      "metadata": {
        "id": "1eW8d7I2r4u9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FFNamZSq3XQ",
        "outputId": "deb7a1f0-fc98-4a85-ed69-6ab87a7f9d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t>>>>>> Progress: 1/28 >>>>>>\n",
            "Accuracy: 100.0%\n",
            "\t>>>>>> Progress: 2/28 >>>>>>\n",
            "Accuracy: 100.0%\n",
            "\t>>>>>> Progress: 3/28 >>>>>>\n",
            "Accuracy: 62.5%\n",
            "\t>>>>>> Progress: 4/28 >>>>>>\n",
            "Accuracy: 75.0%\n",
            "\t>>>>>> Progress: 5/28 >>>>>>\n",
            "Accuracy: 100.0%\n",
            "\t>>>>>> Progress: 6/28 >>>>>>\n",
            "Accuracy: 100.0%\n",
            "\t>>>>>> Progress: 7/28 >>>>>>\n",
            "Accuracy: 87.5%\n",
            "\t>>>>>> Progress: 8/28 >>>>>>\n",
            "Accuracy: 75.0%\n",
            "\t>>>>>> Progress: 9/28 >>>>>>\n",
            "Accuracy: 75.0%\n",
            "\t>>>>>> Progress: 10/28 >>>>>>\n",
            "Accuracy: 87.5%\n",
            "\t>>>>>> Progress: 11/28 >>>>>>\n",
            "Accuracy: 87.5%\n",
            "\t>>>>>> Progress: 12/28 >>>>>>\n",
            "Accuracy: 75.0%\n",
            "\t>>>>>> Progress: 13/28 >>>>>>\n",
            "Accuracy: 75.0%\n",
            "\t>>>>>> Progress: 14/28 >>>>>>\n",
            "Accuracy: 100.0%\n",
            "\t>>>>>> Progress: 15/28 >>>>>>\n",
            "Accuracy: 87.5%\n",
            "\t>>>>>> Progress: 16/28 >>>>>>\n",
            "Accuracy: 50.0%\n",
            "\t>>>>>> Progress: 17/28 >>>>>>\n",
            "Accuracy: 100.0%\n",
            "\t>>>>>> Progress: 18/28 >>>>>>\n",
            "Accuracy: 62.5%\n",
            "\t>>>>>> Progress: 19/28 >>>>>>\n",
            "Accuracy: 87.5%\n",
            "\t>>>>>> Progress: 20/28 >>>>>>\n",
            "Accuracy: 100.0%\n",
            "\t>>>>>> Progress: 21/28 >>>>>>\n",
            "Accuracy: 75.0%\n",
            "\t>>>>>> Progress: 22/28 >>>>>>\n",
            "Accuracy: 87.5%\n",
            "\t>>>>>> Progress: 23/28 >>>>>>\n",
            "Accuracy: 87.5%\n",
            "\t>>>>>> Progress: 24/28 >>>>>>\n",
            "Accuracy: 75.0%\n",
            "\t>>>>>> Progress: 25/28 >>>>>>\n",
            "Accuracy: 87.5%\n",
            "\t>>>>>> Progress: 26/28 >>>>>>\n",
            "Accuracy: 62.5%\n",
            "\t>>>>>> Progress: 27/28 >>>>>>\n",
            "Accuracy: 87.5%\n",
            "\t>>>>>> Progress: 28/28 >>>>>>\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "testing_outputs = []\n",
        "testing_labels = []\n",
        "for i, data in enumerate(train_dataloader, 0):\n",
        "  print(\"\\t>>>>>> Progress: {}/{} >>>>>>\".format(i+1,len(train_dataloader)))\n",
        "  videos, labels = data['videos'], data['labels']\n",
        "  testing_labels.extend(labels)\n",
        "\n",
        "  labels = torch.tensor(F2.one_hot(torch.tensor(labels, dtype=torch.long), num_classes=3), dtype=torch.float32)\n",
        "\n",
        "  test_output = trained_model(videos.to(device))\n",
        "  testing_outputs.append(test_output)\n",
        "  metric = MulticlassAccuracy(num_classes=3)\n",
        "  metric.update(test_output.argmax(dim=1), labels.argmax(dim=1))\n",
        "  print(\"Accuracy: {}%\".format(metric.compute()*100))\n",
        "\n",
        "# testing_outputs = torch.tensor([out.argmax().item() for out in testing_outputs])\n",
        "# testing_labels = torch.tensor([out[0] for out in testing_labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing accuracy of the model on the whole testing dataset"
      ],
      "metadata": {
        "id": "LgxOKVzesGK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "argmaxed_outputs = []\n",
        "for expanded_output in testing_outputs:\n",
        "  argmaxed_outputs.append(expanded_output.argmax(dim=1))"
      ],
      "metadata": {
        "id": "LQEJyT7Gv4lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "argmaxed_outputs = torch.cat(argmaxed_outputs)"
      ],
      "metadata": {
        "id": "OP7K8T7WJD8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_labels = torch.tensor(testing_labels)"
      ],
      "metadata": {
        "id": "a--sK9cGIY8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee-fxkw9q3XQ",
        "outputId": "de3aa653-25ac-40ad-a1c6-08e1072fb1ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8349)\n",
            "tensor([[74.,  0.,  0.],\n",
            "        [ 9., 61.,  0.],\n",
            "        [17., 10., 47.]])\n"
          ]
        }
      ],
      "source": [
        "metric = MulticlassAccuracy(num_classes=3)\n",
        "metric.update(argmaxed_outputs, testing_labels)\n",
        "print(\"Accuracy on the whole test set: \", metric.compute())\n",
        "\n",
        "metric2 = MulticlassConfusionMatrix(3)\n",
        "metric2.update(argmaxed_outputs, testing_labels)\n",
        "print(\"Confusion Matrxi: \", metric2.compute())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the predictions and true labels"
      ],
      "metadata": {
        "id": "m8cs1FXXsa6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the predictions\n",
        "# the path and the name of the file can be changed as required\n",
        "torch.save(argmaxed_outputs,'argmaxed_outputs.pt')"
      ],
      "metadata": {
        "id": "l6DeHg00x0XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the true labels\n",
        "# the path and the name of the file can be changed as required\n",
        "torch.save(testing_labels,'testing_labels.pt')"
      ],
      "metadata": {
        "id": "0aLEf1SUy4Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting model prediction on a gesture video"
      ],
      "metadata": {
        "id": "LgMrDpx3zfyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def more_frames(total_frames, desired_num_frames):\n",
        "        start_offset = math.ceil((total_frames - desired_num_frames)/2)\n",
        "        end_offset = math.floor((total_frames - desired_num_frames)/2)\n",
        "        return start_offset, end_offset"
      ],
      "metadata": {
        "id": "zKfsIjR53GjQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model"
      ],
      "metadata": {
        "id": "R8__8QLo0UuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################# SET THE PATH TO THE SAVED MODEL ##################\n",
        "root_dir = \"/content/drive/MyDrive/gestures_dataset_new/\"\n",
        "path_to_model = os.path.join(root_dir,\"gesture_model_v3.pt\")\n",
        "##############################################################"
      ],
      "metadata": {
        "id": "QHC_fNjc0ett"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the path of the model as required\n",
        "trained_model = torch.load(path_to_model, map_location=torch.device('cpu')) # Use 'cuda' instead of 'cpu' if you saved the model on GPU\n",
        "trained_model = trained_model.to(device)\n",
        "trained_model = trained_model.eval()"
      ],
      "metadata": {
        "id": "GZ5OO1Dxzw7G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the gesture video"
      ],
      "metadata": {
        "id": "smS6AsWY0okw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################# SET THE PATH TO THE GESTURE VIDEO ##################\n",
        "root_dir = \"/content/drive/MyDrive/gestures_dataset_new/\"\n",
        "path_to_video = os.path.join(root_dir, \"train/hand_waving/Wave_20231211_part_1.mp4\")\n",
        "##############################################################\n",
        "\n",
        "frames, _, _ = read_video(path_to_video, output_format=\"TCHW\")\n",
        "total_frames = len(frames)\n",
        "desired_num_frames = 128\n",
        "if (total_frames - desired_num_frames) == 1:\n",
        "    frames = frames[1:]\n",
        "elif (total_frames - desired_num_frames) > 1:\n",
        "    start_offset, end_offset = more_frames(total_frames, 128)\n",
        "    frames = frames[start_offset:-end_offset]\n",
        "elif total_frames < desired_num_frames:\n",
        "    print(\"not enough frames\")\n",
        "    exit(0)\n",
        "\n",
        "# pre-processing the video for the\n",
        "preprocess = weights.transforms()\n",
        "video = preprocess(torch.unsqueeze(frames, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i81XA3k0wcF",
        "outputId": "1902545d-d6df-4ff9-9d87-67584691c2c7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
            "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the prediction"
      ],
      "metadata": {
        "id": "PnucE8oL1Uc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = trained_model(video)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09b0F5Ga1XG0",
        "outputId": "dc25ee07-6ee9-4fdb-a17f-59f856bf890b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_class = {\n",
        "    0: 'hand_waving',\n",
        "    1: 'pointing',\n",
        "    2: 'other'\n",
        "}\n",
        "print(\"Predicted Gesture: \", idx_to_class[prediction.argmax(dim=1).item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvAK_J4E1kie",
        "outputId": "d012e57f-fdbf-450e-fe78-88c466fb15b4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Gesture:  hand_waving\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LbAR_yGQoQAD"
      ]
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}