{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnSXJjcYa6Zr"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SNkAEnJ6a5Xj"
      },
      "outputs": [],
      "source": [
        "#loading libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random, string, argparse, os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('error')\n",
        "from sklearn.exceptions import InconsistentVersionWarning\n",
        "import sklearn\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.io.video import read_video\n",
        "from torchvision.utils import draw_keypoints\n",
        "from torchvision.models.detection import keypointrcnn_resnet50_fpn, KeypointRCNN_ResNet50_FPN_Weights\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.io import read_image\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k1v0n4wZ1zp"
      },
      "source": [
        "### Defining important constants and functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h-lULGMvZVng"
      },
      "outputs": [],
      "source": [
        "nan_values = torch.tensor([float('nan'), float('nan'), float('nan')])\n",
        "\n",
        "coco_keypoints = [\n",
        "    \"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\",\n",
        "    \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\",\n",
        "    \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\",\n",
        "    \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\",\n",
        "]\n",
        "\n",
        "cols = []\n",
        "for kp in coco_keypoints:\n",
        "    cols.append(kp + \"_x\")\n",
        "    cols.append(kp + \"_y\")\n",
        "\n",
        "keypoints_of_interest = [\n",
        "     \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\", \"left_wrist\", \"right_wrist\"\n",
        "]\n",
        "\n",
        "connections_of_interest = {\n",
        "    \"left_upper_arm\" : [\"left_shoulder\", \"left_elbow\"],\n",
        "    \"left_lower_arm\" : [\"left_elbow\", \"left_wrist\"],\n",
        "    \"right_upper_arm\" : [\"right_shoulder\", \"right_elbow\"],\n",
        "    \"right_lower_arm\" : [\"right_elbow\", \"right_wrist\"]\n",
        "}\n",
        "\n",
        "data_attr_of_interest = [\n",
        "    kp + \"_\" + coord\n",
        "    for kp in keypoints_of_interest\n",
        "    for coord in [\"x\",\"y\"]\n",
        "]\n",
        "\n",
        "connection_combinations = list(combinations(connections_of_interest.keys(), 2))\n",
        "\n",
        "def dot(vA, vB):\n",
        "    return vA[0]*vB[0]+vA[1]*vB[1]\n",
        "\n",
        "def make_short_form(name):\n",
        "  return \"\".join([word[0].upper() for word in name.split(\"_\")])\n",
        "\n",
        "def to_ij_vector(coordinates):\n",
        "  return [(round(coordinates[1][0]-coordinates[0][0],4)), (round(coordinates[1][1]-coordinates[0][1],2))]\n",
        "\n",
        "def ang(vA, vB):\n",
        "    try:\n",
        "      # Get dot prod\n",
        "      dot_prod = dot(vA, vB)\n",
        "      # Get magnitudes\n",
        "      magA = dot(vA, vA)**0.5\n",
        "      magB = dot(vB, vB)**0.5\n",
        "      # Get cosine value\n",
        "      cos_ = dot_prod/magA/magB\n",
        "      # Get angle in radians and then convert to degrees\n",
        "      angle = math.acos(dot_prod/magB/magA)\n",
        "      # Basically doing angle <- angle mod 360\n",
        "      ang_deg = math.degrees(angle)%360\n",
        "\n",
        "      if ang_deg-180>=0:\n",
        "          # As in if statement\n",
        "          return 360 - ang_deg\n",
        "      else:\n",
        "\n",
        "          return ang_deg\n",
        "    except Exception as e:\n",
        "      print(e, \":\", vA, vB)\n",
        "      return 0\n",
        "\n",
        "# Function to spit out features given frames and keypoints\n",
        "def get_features(df):\n",
        "  data = df[data_attr_of_interest].dropna()\n",
        "\n",
        "  for val in keypoints_of_interest:\n",
        "    data[val] = list(zip(data[f\"{val}_x\"], data[f\"{val}_y\"]))\n",
        "\n",
        "  for connection, (keypoint1, keypoint2) in connections_of_interest.items():\n",
        "    data[connection] = list(zip(data[keypoint1], data[keypoint2]))\n",
        "\n",
        "  # Converting to i j vectors\n",
        "  for connection in connections_of_interest:\n",
        "    data[connection] = data[connection].apply(to_ij_vector)\n",
        "\n",
        "  # Connection vector angles\n",
        "  rest_pos_vector = [0,-1] # downward, -j unit vector\n",
        "\n",
        "  for connection in connections_of_interest:\n",
        "    data[connection + \"_angle\"] = data[connection].apply(ang, vB=rest_pos_vector)\n",
        "\n",
        "  # Feature - Ratio of angles lower angle:higher angle of anlge between left upper and lower arm and for right\n",
        "  ratios = []\n",
        "\n",
        "  for idx, row in data.iterrows():\n",
        "    left_arm_angle = ang(row[\"left_upper_arm\"], row[\"left_lower_arm\"])\n",
        "    right_arm_angle = ang(row[\"right_upper_arm\"], row[\"right_lower_arm\"])\n",
        "    ratios.append(round(min([left_arm_angle, right_arm_angle])*100/max([left_arm_angle, right_arm_angle]),4))\n",
        "\n",
        "  data[\"arm_angle_ratio\"] = ratios\n",
        "\n",
        "  # Ratio of y of wrists positions\n",
        "  ratios = []\n",
        "\n",
        "  for idx, row in data.iterrows():\n",
        "    min_y = min([row[\"left_wrist_y\"], row[\"right_wrist_y\"]])\n",
        "    max_y = max([row[\"left_wrist_y\"], row[\"right_wrist_y\"]])\n",
        "    ratios.append(round(min_y*100/max_y,4))\n",
        "\n",
        "  data[\"wrist_y_ratio\"] = ratios\n",
        "\n",
        "  # Difference of angle between each connection combination\n",
        "  for connection1, connection2 in connection_combinations:\n",
        "    data[f\"{make_short_form(connection1)}_{make_short_form(connection2)}\"] = (data[connection1 + \"_angle\"] - data[connection2 + \"_angle\"]).abs()\n",
        "\n",
        "  # Ratio of y of wrist vs shoulder positions\n",
        "  ratios = []\n",
        "\n",
        "  for idx, row in data.iterrows():\n",
        "    left = abs(row[\"left_wrist_y\"] - row[\"left_shoulder_y\"])\n",
        "    right = abs(row[\"right_wrist_y\"] - row[\"right_shoulder_y\"])\n",
        "    ratios.append(round(min([left,right])*100/max([left,right]),4))\n",
        "\n",
        "  data[\"wrist_shoulder_y_ratio\"] = ratios\n",
        "\n",
        "  # Start summarizing\n",
        "  starting_col_idx = data.columns.tolist().index(\"left_upper_arm_angle\")\n",
        "\n",
        "  features_video = {}\n",
        "\n",
        "  for col in data.columns[starting_col_idx:]:\n",
        "    features_video[f\"{col}_min\"] = data[col].min().tolist()\n",
        "    features_video[f\"{col}_max\"] = data[col].max().tolist()\n",
        "    features_video[f\"{col}_mean\"] = data[col].mean().tolist()\n",
        "    features_video[f\"{col}_median\"] = data[col].median().tolist()\n",
        "    features_video[f\"{col}_var\"] = data[col].var().tolist()\n",
        "\n",
        "  return pd.Series(features_video).sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoUv1m7tbXk8"
      },
      "source": [
        "### Loading the Keypoint Detection model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7Nx9-xOau8s",
        "outputId": "f0db3bf0-12f0-462e-fcd5-a4a41ca75fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Loading the model\n",
            "===> The keypoint detector model loaded!\n"
          ]
        }
      ],
      "source": [
        "#Loading the model\n",
        "print(\"===> Loading the model\")\n",
        "weights = KeypointRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "transforms = weights.transforms()\n",
        "\n",
        "model = keypointrcnn_resnet50_fpn(weights=weights, progress=False)#.to(device)\n",
        "model = model.eval()\n",
        "print(\"===> The keypoint detector model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEMx5vZYbknV"
      },
      "source": [
        "### Loading the video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkszdJnvbyIB"
      },
      "source": [
        "***** Please provide the path to the video *****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6SOb6gd9buve"
      },
      "outputs": [],
      "source": [
        "path_to_video = \"/Users/amankumar/Work/ECE_699/dataset_new/train/hand_waving/Wave_20231211_part_1.mp4\"\n",
        "\n",
        "####### You can also change the interval between the extracted frame ######\n",
        "step_between_frames = 3\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pS2fVihbpTN",
        "outputId": "c9527a46-0cbd-4705-99be-abfe031f1dd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Loading the video\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/amankumar/miniforge3/envs/deep-learning/lib/python3.8/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
            "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video metadata:  {'video_fps': 30.0, 'audio_fps': 44100}\n",
            "Total number of frames extracted:  150\n",
            "Total number of frames selected:  50\n"
          ]
        }
      ],
      "source": [
        "print(\"===> Loading the video\")\n",
        "frames, t1, t2 = read_video(path_to_video, output_format=\"TCHW\")\n",
        "print(\"Video metadata: \", t2)\n",
        "print(\"Total number of frames extracted: \", len(frames))\n",
        "# Fetching selected frames\n",
        "#gesture_int = [frames[x].to(device) for x in range(0,len(frames),step_between_frames)]\n",
        "gesture_int = [frames[x] for x in range(0,len(frames), step_between_frames)]\n",
        "print(\"Total number of frames selected: \", len(gesture_int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rHVjSsXc3y1"
      },
      "source": [
        "### Detecting keypoints and saving the keypoints into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGzlg5JDc-U4",
        "outputId": "65d91025-87ed-4840-90d7-e1cb3a965333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Preprocessing the frames for the model\n",
            "===> Processing frames to detect keypoints\n",
            "===> The video is processed\n"
          ]
        }
      ],
      "source": [
        "detect_threshold = 0.75\n",
        "print(\"===> Preprocessing the frames for the model\")\n",
        "# gestures_float = [transforms(img.to(device)) for img in gesture_int]\n",
        "gestures_float = [transforms(img) for img in gesture_int]\n",
        "\n",
        "# Detecting the keypoints in the frames\n",
        "print(\"===> Processing frames to detect keypoints\")\n",
        "#with torch.no_grad():\n",
        "outputs = []\n",
        "for gesture in gestures_float:\n",
        "    outputs.append(model([gesture])[0])\n",
        "#outputs = model(gestures_float)\n",
        "print(\"===> The video is processed\")\n",
        "\n",
        "# Keeping the keypoints of those objects from the video whose confidence score was above threshold\n",
        "for i in range(len(outputs)):\n",
        "    idx = torch.where(outputs[i]['scores'] > detect_threshold)\n",
        "    #if the keypoint is not available, then making it nan\n",
        "    if len(idx[0]) == 0:\n",
        "        print(\"====> No person detected. Skip and Go ahead! <====\")\n",
        "        outputs[i]['keypoints'][0] = nan_values\n",
        "    else:\n",
        "        outputs[i]['keypoints'] = outputs[i]['keypoints'][idx]\n",
        "        outputs[i]['keypoints_scores'] = outputs[i]['keypoints_scores'][idx]\n",
        "        # Also, filling nan values for keypoints whose confidence score is in negative (=> couldn't find that joint)\n",
        "        outputs[i]['keypoints'][0][torch.where(outputs[i]['keypoints_scores'][0] < 0)] = nan_values\n",
        "\n",
        "# formatting the data so that it can be saved as DataFrame csv\n",
        "data_csv = [outputs[idx]['keypoints'][0][:,0:2].detach().numpy().flatten() for idx in range(0,len(outputs))]\n",
        "\n",
        "keypoints = pd.DataFrame(data=data_csv, columns=cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sEmVLEweHQf"
      },
      "source": [
        "### Performing feature engineering on the keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8iQH7B4QdA3T"
      },
      "outputs": [],
      "source": [
        "test_x = get_features(keypoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz3ZDYMAeQ3n"
      },
      "source": [
        "### Loading the machine learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***** Please provide the path to the model *****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "A2PQmk81ipLS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model was pickled in different scikit-learn version than your current scikit-learn version of 1.3.2. \n",
            "Due to difference in versions, there might be some inconsistencies while unpickling the model. \n",
            "To make sure that the model behaves as expected, try using the compatible version scikit-learn version mentioned in requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# Location of model\n",
        "model_file_path = \"/Users/amankumar/Work/ECE_699/gesture-recognition/trained_models/feature_engineering_model.sklearn-1-2-2.pickle\"\n",
        "\n",
        "# Load the saved model pipeline\n",
        "try:\n",
        "  with open(model_file_path, 'rb') as pickle_file:\n",
        "    loaded_pipeline = pickle.load(pickle_file)\n",
        "except InconsistentVersionWarning:\n",
        "  print(\"The model was pickled in different scikit-learn version than your current scikit-learn version of {}. \\nDue to difference in versions, there might be some inconsistencies while unpickling the model. \\nTo make sure that the model behaves as expected, try using the compatible version scikit-learn version mentioned in requirements.txt\".format(sklearn.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou3FxvQcehfW"
      },
      "source": [
        "### Getting the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M6klPGzEida9"
      },
      "outputs": [],
      "source": [
        "# Predict for the test set\n",
        "prediction = loaded_pipeline.predict([test_x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1NAafWRJe5J8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Gesture:  ['hand_waving']\n"
          ]
        }
      ],
      "source": [
        "print(\"Predicted Gesture: \", prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
